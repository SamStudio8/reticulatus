import pandas as pd

# Import config, set working dir, load manifest and define default rules:
include: "Snakefile-base"

references = pd.read_table("../ref.cfg").set_index(["refgroup", "refname"], drop=False)
GENOMES = True

rule finish_zymo:
    input:
        base="reticulated.base.ok", # This will ensure the base rules have finished first
        #checkm=expand("checkm-{assembly}.txt", assembly=enumerate_assemblies(base_only=False, unroll=True, suffix=".fa")),
        fastmer="fastmer-result.txt",
        reads_mapped=enumerate_reads(suffix=".map.stat.summary.tsv", cols=["ont"], refgroup=True),
    output: touch("reticulated.zymo.ok")
    shell: 'echo "Reticulated successfully."'

def retic_get_ref(refgroup, genome):
    r = references.loc[refgroup]
    refname = r["refname"].get(genome, 'default')
    refloc = r.loc[refname]["ref"] if r.loc[refname]["ref"] != '-' else None
    if refloc and refloc.startswith("HTTP"):
        refloc = "download/refs/%s/%s.fa" % (refgroup, refname)
    return refloc

#from snakemake.remote.HTTP import RemoteProvider as HTTPRemoteProvider
#HTTP = HTTPRemoteProvider()
rule download_ref:
    #input: lambda w: HTTP.remote(references.loc[w.ref.replace(".fa", "")]["ref"].replace("HTTP:", ""), keep_local=True)
    params:
        remote=lambda w: references.loc[w.refgroup, w.ref]["ref"].replace("HTTP:", "")
    output: "download/refs/{refgroup,[A-z0-9_-]*}/{ref}.fa"
    run:
        shell("wget {params.remote} -O {output}")

def get_refgroup_by_uuid(uuid):
    return samples.loc[uuid]["refgroup"]

def get_ref_paths_by_group(refgroup):
    return [retic_get_ref(refgroup, ref) for ref in references.loc[refgroup]["refname"].values.tolist()]

def get_ref_names_by_group(refgroup):
    return references.loc[refgroup]["refname"].values.tolist() # ffs pandas

def get_ref_tids_by_group(refgroup):
    return references.loc[refgroup]["ncbi_tid"].values.tolist()

#TODO WIll want to perhaps genericise this for multiple ref sets
rule merge_refs:
    input: lambda w: get_ref_paths_by_group(w.refgroup)
    output: "{refgroup,[A-z0-9_-]*}.super_ref.fa"
    params:
        refnames=lambda w: get_ref_names_by_group(w.refgroup)
    shell: "REFS=({input}); NAMES=({params.refnames}); for ((i=0;i<${{#REFS[@]}};++i)); do sed \"s,^>,>${{NAMES[i]}}__,\" ${{REFS[i]}}; done > {output}"

rule map_reads:
    input:
        ref="{refgroup}.super_ref.fa",
        reads="{path}"
    params:
        samplename=lambda w: reads_lookup.loc[reads_lookup['ont'] == w.path]["samplename"][0],
    output:
        "{path}.{refgroup,[A-z0-9_-]*}.map.stat"
    threads: lambda w: n_threads(None, "minimap2_threads")
    shell: "minimap2 -a -2 --sam-hit-only --secondary=no -Q -t {threads} -x map-ont {input.ref} {input.reads} | python ../scripts/zymo/nickloman/bamstats.py - | awk '{{print \"{params.samplename}\t\" \"{wildcards.refgroup}\t\" $0}}' | bond - ../reads.cfg --dheader samplename,refgroup,refname,contig,readid,alen --dropid | bond - ../ref.cfg --dcol 2,3 --mcol 1,2 --dropid > {output}"

rule map_reads_summary:
    conda: "environments/r.yaml"
    input: "{path}.{refgroup}.map.stat"
    output: "{path}.{refgroup,[A-z0-9_-]*}.map.stat.sumr.tsv"
    shell: "Rscript ../scripts/zymo/nickloman/summariseStats.R {input} {output}"
rule bond_map_reads_summary:
    input: "{path}.{refgroup}.map.stat.sumr.tsv"
    output: "{path}.{refgroup,[A-z0-9_-]*}.map.stat.summary.tsv"
    shell: "bond {input} ../reads.cfg --dropid > {output}"

rule extract_kraken_contigs:
    input:
        assembly="{uuid}.{assembly}",
        k2="{uuid}.{assembly}.k2",
    params:
        refnames=lambda w: get_ref_names_by_group(get_refgroup_by_uuid(w.uuid))
    output:
        directory("extracted_contigs/{uuid,[A-z0-9_-]*}.{assembly}/")
    shell:
        "python ../scripts/extract_contigs_with_kraken.py {input.k2} {input.assembly} {output}; python ../scripts/zymo/ensure_genomes.py {output} {params.refnames}"

#rule presetup_checkm:
#    output: directory("checkm")
#    shell: "mkdir checkm; cd checkm; wget https://data.ace.uq.edu.au/public/CheckM_databases/checkm_data_2015_01_16.tar.gz; tar xvf checkm_data_2015_01_16.tar.gz"
#
#rule setup_checkm:
#    conda: "environments/checkm.yaml"
#    input: directory("checkm")
#    output: touch("checkm_setup.ok")
#    shell: "echo 'checkm/' | checkm data setRoot 'checkm/'"
#
#def checkm_pick_taxon(w):
#    refname =  references["refname"].get(w.genome, 'default')
#    return {
#        "rank": references.loc[refname]["rank"],
#        "genome": references.loc[refname]["taxon"]
#    }
#
## This rule is a little annoying, we *should* be explicitly naming the input
## FASTA needed for the checkM bin as extracted by our kraken script.
## Unfortunately, as the extract_kraken_contigs rule output is defined as a dictionary
## we cannot explicitly name an input inside that directory without a ChildIOException.
## That rule has a python script that attempts to guarantee those files exist, so
## as long as the directory exists, we can be reasonably confident the FASTA do too.
##   As gross as it sounds, the requisite `cp` will fail if it *really* doesn't exist.
## So this isn't a particularly unsafe workaround, just a gross one.
#rule checkm:
#    input:
#        ok="checkm_setup.ok",
#        d=directory("extracted_contigs/{assembly}/")
#        #fa="extracted_contigs/{assembly}/{genome}.fasta"
#    output:
#        working=directory("checkm-working/{assembly}/{genome}"),
#        bin=directory("checkm-bins/{assembly}/{genome}/"),
#        res="checkm-results/{assembly}/{genome}.tsv"
#    params: p=checkm_pick_taxon
#    conda: "environments/checkm.yaml"
#    threads: 8
#    shell: "cp extracted_contigs/{wildcards.assembly}/{wildcards.genome}.fasta {output.bin}; checkm taxonomy_wf -q -t {threads} -x fasta {params.p[rank]} {params.p[genome]:q} {output.bin} {output.working} > {output.res}"
#
#rule merge_checkm:
#    input:
#        logs=expand("checkm-results/{{assembly}}/{genome}.tsv", genome=GENOMES)
#    output:
#        "checkm-{assembly}.txt"
#    shell:
#        "cat {input.logs} > {output}"

rule jts_fastmer:
    input:
        reference=lambda w: retic_get_ref(get_refgroup_by_uuid(w.uuid), w.genome),
        assembly="{uuid}.{assembly}",
    output:
        res="fastmer-results/{uuid,[A-z0-9_-]*}.{assembly}/{genome}.tsv",
        bam=temp("{uuid,[A-z0-9_-]*}.{assembly}.{genome}.assembly_analysis.sorted.bam"),
        bai=temp("{uuid,[A-z0-9_-]*}.{assembly}.{genome}.assembly_analysis.sorted.bam.bai"),
    shell: "python ../scripts/zymo/jts/fastmer.py --temp-bam {output.bam} --reference {input.reference} --assembly {input.assembly} --min-mapping-quality 10 > {output.res}"

rule merge_fastmer:
    input:
        #logs=lambda w: expand("fastmer-results/{{uuid}}.{{assembly}}/{genome}.tsv", genome=[ref for ref in get_ref_names_by_group(get_refgroup_by_uuid(w.uuid)) if retic_get_ref(get_refgroup_by_uuid(w.uuid), ref)])
        logs=lambda w: ["fastmer-results/{uuid}.{assembly}/%s.tsv" % g for g in [ref for ref in get_ref_names_by_group(get_refgroup_by_uuid(w.uuid)) if get_refgroup_by_uuid(w.uuid) != '-' and retic_get_ref(get_refgroup_by_uuid(w.uuid), ref)]]
    output:
        "fastmer-{uuid,[A-z0-9_-]*}.{assembly}.txt"
    shell:
        "cat {input.logs} | awk '!/^assembly_name/ || ++n < 2' > {output}"

rule super_merge_fastmer:
    input:
        logs=expand("fastmer-{uuid}.{assembly}.txt", zip, uuid=[x.split('.')[0] for x in enumerate_assemblies(base_only=False, unroll=True, suffix=".fa", refgroup=True)], assembly=[x.split('.', 1)[1] for x in enumerate_assemblies(base_only=False, unroll=True, suffix=".fa", refgroup=True)])
    output:
        "fastmer-result.txt"
    shell:
        "cat {input.logs} | awk '!/^assembly_name/ || ++n < 2' | bond - ../manifest.cfg --greedy | bond - ../reads.cfg --dcol 18 > {output}"
