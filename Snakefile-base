import pandas as pd
from snakemake.utils import validate
import sys
import time
from spellbook import spells

shell.executable('bash')

# Import config
configfile: "config.yaml"

# Set working dir to "working/", you can change this to anything.
# The rationale is you can git clone the pipeline and run it without doing any juggling.
# Note that most things need to be prefixed with ../ to refer to the current directory,
# but there are exceptions like when reading cfg files and conda environments.
workdir: "working/"

onsuccess:
    # TODO Need to update https://github.com/slackapi/python-slackclient/wiki/Migrating-to-2.x
    cwd = os.getcwd()
    if config["slack_token"]:
        try:
            from slackclient import SlackClient

            token = config["slack_token"]
            sc = SlackClient(token)
            sc.api_call(
                    "chat.postMessage", channel=config["slack_channel"],
                    attachments= [{
                        "title": "Snakemake pipeline completed successfully",
                        "color": "#36a64f",
                        "text": "Hooray!",
                        "footer": "Completed pipeline spotted by SnakeEyes",
                        "footer_icon": "https://avatars.slack-edge.com/2019-05-07/627979128852_e1d616abbd7312343db8_512.png",
                        "ts": int(time.time()),
                        "fields": [
                            { "title": "Working Directory", "value": cwd, "short": False },
                        ],
                    }]
            )
        except ModuleNotFoundError:
            pass

onerror:
    cwd = os.getcwd()
    if config["slack_token"]:
        try:
            from slackclient import SlackClient

            token = config["slack_token"]
            sc = SlackClient(token)
            sc.api_call(
                    "chat.postMessage", channel=config["slack_channel"],
                    attachments= [{
                        "title": "Snakemake pipeline terminated unexpectedly",
                        "color": "#e20b00",
                        "text": "Boo :(",
                        "footer": "Trashed pipeline spotted by SnakeEyes",
                        "footer_icon": "https://avatars.slack-edge.com/2019-05-07/627979128852_e1d616abbd7312343db8_512.png",
                        "ts": int(time.time()),
                        "fields": [
                            { "title": "Working Directory", "value": cwd, "short": False },
                        ],
                    }]
            )
        except ModuleNotFoundError:
            pass

# Load manifest(s)
samples = pd.read_table("../manifest.cfg").set_index("uuid", drop=False)  # defines the assembly and polishing strategies
reads_lookup = pd.read_table("../reads.cfg").set_index("samplename", drop=False)

# Utility functions
def unroll_assemblies(w, name, unroll=True):
    if not unroll:
        return [name]

    polishes = name.split(".")
    if len(polishes) == 0 or len(polishes[0]) == 0:
        return []

    end_polish = polishes[-1]
    end_polish_f = end_polish.split("-")
    end_iter = int(end_polish_f[-1])
    if (end_iter - 1) < 1:
        return [name] + unroll_assemblies(w, ".".join(polishes[:-1]))
    else:
        end_polish_f[-1] = str(end_iter-1)
        return [name] + unroll_assemblies(w, ".".join(polishes[:-1] + ["-".join(end_polish_f)]))

def enumerate_assemblies(w=None, suffix="", unroll=True, base_only=False):
    base_assemblies = []
    polished_assemblies = []
    for uuid in samples["uuid"]:
        conf = samples.loc[uuid]["spell"]
        #base_uuid = uuid
        #if samples.loc[uuid]["useassembly"]:
        #    base_uuid = samples.loc[uuid]["useassembly"]
        base_assemblies.append( "%s.%s.ctg.cns%s" % (uuid, conf, suffix) )

        if samples.loc[uuid]["polishpipe"] != '-' and not base_only:
            unrolled = unroll_assemblies(w, samples.loc[uuid]["polishpipe"], unroll=unroll)
            context = ['%s.%s.ctg.cns.%s%s' % (uuid, conf, step, suffix) for step in unrolled]
            polished_assemblies.extend(context)
    return base_assemblies + polished_assemblies

# TODO samstudio8
# This function is fired during construction of the DAG when searching the parameter space of the GPU jobs,
# it might be worth trying to catch the fact that execution hasn't started yet to suppress this output?
def select_gpu_device(wildcards, resources):
    if not config["cuda"]:
        return None
    import GPUtil
    available = ",".join([str(x) for x in GPUtil.getAvailable(order = 'random', limit = resources.gpu, maxLoad = 0.5, maxMemory = 0.5, includeNan=False, excludeID=[], excludeUUID=[])])
    print("Assigning %d available GPU devices: %s" % (resources.gpu, available))
    return available


for a in enumerate_assemblies(suffix=".fa"):
    sys.stderr.write("*\t%s\n" % a)

rule finish:
    input:
        stat="assembly_stats.txt",
        meta="assembly_md5size.txt",
        kraken="kraken_summary.bond.tsv",
        base_graphs=enumerate_assemblies(unroll=False, base_only=True, suffix=".gfa.svg"),
        polished=enumerate_assemblies(base_only=False, unroll=False, suffix=".fa"),

rule merge_reads:
    input: lambda w: get_reads(w, w.readtype, samplename=os.path.basename(w.samplename))
    output:
        "{samplename}.cat-{readtype,\w+}.{ext,(fq|fastq)(\.gz|)}"
    shell:
        "cat {input} > {output}"
    

# new and improved version of this rule doesn't fucking nuke your input reads
rule subset_reads:
    input:
        "{fq}.{ext}"
    output:
        "{fq}.subset-{ratio,\d+}.{ext,(fq|fastq)(\.gz|)}"
    params:
        ratio=lambda w: float(w.ratio)/100
    shell:
        "seqkit sample {input} -p {params.ratio} -o {output}"

rule rmdup_reads:
    input:
        "{fq}.{ext}"
    output:
        out="{fq}.rmdup.{ext,(fq|fastq)(\.gz|)}",
        dup="{fq}.dup.{ext,(fq|fastq)(\.gz|)}",
    shell:
        "seqkit rmdup -n -i {input} -o {output.out} -d {output.dup}"

def input_polish(w, name):
    if int(w.iteration) > 1:
        contigs = "%s.%s.ctg.cns.%s%s-%s-%d.fa" % (w.uuid, w.conf, w.polishedprefix, name, w.readtype, int(w.iteration)-1)
    elif w.polishedprefix=="":
        contigs = "%s.%s.ctg.cns.fa" % (w.uuid, w.conf)
    else:
        contigs = "%s.%s.ctg.cns.%sfa" % (w.uuid, w.conf, w.polishedprefix)
    return contigs

def input_polish_racon(w):
    return input_polish(w, "racon")
def input_polish_medaka(w):
    return input_polish(w, "medaka")
def input_polish_pilon(w):
    return input_polish(w, "pilon")
def input_polish_dehumanizer(w):
    return input_polish(w, "dehumanizer")

def minimap2_mode(w):
    if w.readtype == "ont":
        mode = "map-ont"
    elif w.readtype == "ill":
        mode = "sr"
    return mode


def get_reads(w, readtype, samplename=None):
    if samplename:
        r = reads_lookup.loc[samplename][readtype]
	return r.split(':')[1].split(',')

    r = reads_lookup.loc[ samples.loc[w.uuid]['samplename'] ][readtype]
    if ':' in r:
        r = os.path.join(r.split(':')[0], '%s.cat-%s.fq.gz' % (samples.loc[w.uuid]['samplename'], readtype))
    return r

def polish_reads_input(w):
    if w.readtype == "ont":
        reads = get_reads(w, 'ont')
    elif w.readtype == "ill":
        reads = [get_reads(w, 'i1'), get_reads(w, 'i2')]
    return reads

rule minimap2_racon:
    input: contigs=input_polish_racon, reads=polish_reads_input
    params:
        mode=minimap2_mode,
        #reads=lambda w, input: input.reads.replace(".gz", ".b.gz")
    threads: config["minimap2_threads"]
    output:
        temp("{uuid}.{conf}.ctg.cns.{polishedprefix,.*}racon-{readtype,\w+}-{iteration,\d+}.fa.sam")
    benchmark: "benchmarks/{uuid}.{conf}.ctg.cns.{polishedprefix,.*}racon-{readtype,\w+}-{iteration,\d+}.fa.sam"
    shell: "minimap2 -K2G -a -2 -Q --sam-hit-only -t {threads} -x {params.mode} {input.contigs} {input.reads} > {output}"
    #shell: "cudamapper -w 10 -t 1000 -i 5000 {params.reads} {input.contigs} > {output}"


rule polish_racon:
    input: contigs=input_polish_racon, reads=polish_reads_input, overlaps="{uuid}.{conf}.ctg.cns.{polishedprefix,.*}racon-{readtype,\w+}-{iteration,\d+}.fa.sam",
    params:
        cuda="--cudapoa-batches %d" % config["racon_batches"] if config["cuda"] else "",
        devices=select_gpu_device,
    output:
        "{uuid}.{conf}.ctg.cns.{polishedprefix,.*}racon-{readtype,\w+}-{iteration,\d+}.fa"
    benchmark: "benchmarks/{uuid}.{conf}.ctg.cns.{polishedprefix,.*}racon-{readtype,\w+}-{iteration,\d+}.fa"
    threads: config["polish_threads"]
    resources:
        gpu=config["polish_gpu"] if config["cuda"] else 0,
    shell:
        "export CUDA_VISIBLE_DEVICES={params.devices}; racon -m 8 -x -6 -g -8 -w 500 -t {threads} {input.reads} {input.overlaps} {input.contigs} {params.cuda} > {output}"

rule polish_medaka:
    singularity: config["medaka_env"],
    input: contigs=input_polish_medaka, reads=polish_reads_input
    params:
        prefix=lambda w: w.uuid+'.'+w.conf,
        model=lambda w: samples.loc[w.uuid]['medakamodel'],
        batch="-b 100" if config["cuda"] else "",
        devices=select_gpu_device,
    output:
        "{uuid}.{conf}.ctg.cns.{polishedprefix,.*}medaka-{readtype,\w+}-{iteration,\d+}.fa"
    threads: config["polish_threads"]
    benchmark: "benchmarks/{uuid}.{conf}.ctg.cns.{polishedprefix,.*}medaka-{readtype,\w+}-{iteration,\d+}.fa"
    resources:
        gpu=config["polish_gpu"] if config["cuda"] else 0,
    shell:
        "export CUDA_VISIBLE_DEVICES={params.devices}; export TF_FORCE_GPU_ALLOW_GROWTH=true; rm -rf medaka-{params.prefix}/*; medaka_consensus -i {input.reads} -d {input.contigs} -o medaka-{params.prefix} -t {threads} -m {params.model} {params.batch}; mv medaka-{params.prefix}/consensus.fasta {output}"

rule download_dehumanizer_database:
    output:
        ok=touch(os.path.join(config["dehumanizer_database_root"], "dehuman.ok")),
        a=os.path.join(config["dehumanizer_database_root"], "GCA_000786075.2_hs38d1_genomic.fna.mmi"),
        b=os.path.join(config["dehumanizer_database_root"], "GCA_000001405.27_GRCh38.p12_genomic.fna.mmi"),
        c=os.path.join(config["dehumanizer_database_root"], "hla_gen.fasta.mmi"),
        manifest=os.path.join(config["dehumanizer_database_root"], "manifest.txt")
    shell:
        "bash ../scripts/download_dehumanizer_refs.sh %s" % config["dehumanizer_database_root"]

rule polish_dehumanizer:
    input:
        contigs=input_polish_dehumanizer,
        ok=os.path.join(config["dehumanizer_database_root"], "dehuman.ok"),
        manifest=os.path.join(config["dehumanizer_database_root"], "manifest.txt")
    output:
        "{uuid}.{conf}.ctg.cns.{polishedprefix,.*}dehumanizer-{readtype,\w+}-{iteration,\d+}.fa"
    threads: 2
    shell:
        "dehumanize {input.manifest} {input.contigs} {output}"

rule download_pilon:
    output: "pilon-1.23.jar"
    shell: "wget https://github.com/broadinstitute/pilon/releases/download/v1.23/pilon-1.23.jar"

rule polish_pilon:
    input: contigs=input_polish_pilon, reads=polish_reads_input, pilon="pilon-1.23.jar"
    params:
        mode=minimap2_mode,
    output:
        polish="{uuid}.{conf}.ctg.cns.{polishedprefix,.*}pilon-{readtype,\w+}-{iteration,\d+}.fa"
    threads: config["polish_threads"]
    benchmark: "benchmarks/{uuid}.{conf}.ctg.cns.{polishedprefix,.*}pilon-{readtype,\w+}-{iteration,\d+}.fa"
    shell:
        "echo {input.reads}; minimap2 -t {threads} -ax {params.mode} {input.contigs} {input.reads} > {output}.sam; samtools sort {output}.sam -T {wildcards.uuid} -m 2G -@ {threads} -o {output}.bam; samtools index {output}.bam; java -Xmx16G -jar {input.pilon} --genome {input.contigs} --bam {output}.bam --outdir pilon-{wildcards.uuid}/; mv pilon-{wildcards.uuid}/pilon.fasta {output}"

rule summarise_assembly_stats:
    input:
        enumerate_assemblies(suffix=".fa")
    output:
        "assembly_stats.txt"
    shell:
        "for fa in {input}; do perl ../scripts/assembly-stats.pl $fa; done > {output}"

rule summarise_assembly_meta:
    input:
        enumerate_assemblies(suffix=".fa")
    output:
        "assembly_md5size.txt"
    shell:
        "for fa in {input}; do base=`basename $fa`; md5=`md5sum $fa | cut -f1 -d' '`; size=`du -Lh $fa | cut -f1`; echo \"$base,$size,$md5\"; done > {output}"

rule bond_summarise_kraken:
    input:
        "kraken_summary.tsv"
    output:
        "kraken_summary.bond.tsv"
    shell:
        "bond {input} ../manifest.cfg > {output}"

rule summarise_kraken:
    input:
        enumerate_assemblies(suffix=".fa.k2")
    output:
        "kraken_summary.tsv"
    shell:
        "python ../scripts/extracken2.py {input} > {output}"

rule download_kraken_database:
    output:
        ok=touch(os.path.join(config["kraken2_database_root"], "k2db.ok")),
        h=os.path.join(config["kraken2_database_root"], "hash.k2d"),
        o=os.path.join(config["kraken2_database_root"], "opts.k2d"),
        t=os.path.join(config["kraken2_database_root"], "taxo.k2d"),
    shell: "bash ../scripts/download_kraken2_db.sh %s" % config["kraken2_database_root"]

rule kraken:
    input:
        fa="{uuid}.{prefix}.fa", ok=os.path.join(config["kraken2_database_root"], "k2db.ok")
    output:
        out="{uuid}.{prefix}.fa.k2",
        rep="{uuid}.{prefix}.fa.k2r",
    threads: 8
    benchmark: "benchmarks/{uuid}.{prefix}.fa.k2"
    shell:
        "kraken2 --db %s --use-names -t {threads} --output {output.out} --report {output.rep} {input.fa}" % config["kraken2_database_root"]


# TODO We need to use git/Flye/bin/flye as the script sets up some dir dependent stuff
rule install_flye_hash:
    output:
        ok=touch("{conf,flye[A-z0-9_-]*}.ok"),
        #flye_bin="git/{conf,flye[A-z0-9_-]*}/Flye/bin/flye",
    params:
        hash=lambda w: spells[w.conf]["hash"],
    shell: "mkdir -p git/{wildcards.conf}; cd git/{wildcards.conf}; git clone https://github.com/fenderglass/Flye.git; cd Flye; git reset --hard {params.hash}; make"

rule flye_assembly:
    conda: "environments/flye.yaml"
    input:
        reads=lambda w: get_reads(w, 'ont'),
        ready=lambda w: "%s.ok" % samples.loc[w.uuid]["spell"],
    params:
        overlap=lambda w: '-m %s' % spells[w.conf]['m'] if spells[w.conf]['m'] != '-' else '',
        plasmids=lambda w: '--plasmids' if spells[w.conf]['plasmids'] else '',
        genome_size=lambda w: spells[w.conf]['genome_size'],
        d = "{uuid}.{conf,flye[A-z0-9_-]*}/",
        iterations=lambda w: '--iterations %s' % spells[w.conf]['iterations'] if 'iterations' in spells[w.conf] else '',
    output:
        fa = "{uuid}.{conf,flye[A-z0-9_-]*}/assembly.fasta",
        gfa = "{uuid}.{conf,flye[A-z0-9_-]*}/assembly_graph.gfa"
    threads: config["assembly_threads"]
    benchmark: "benchmarks/{uuid}.{conf,flye[A-z0-9_-]*}_assembly.fa"
    shell:
        "git/{wildcards.conf}/Flye/bin/flye --nano-raw {input.reads} --meta {params.plasmids} -g {params.genome_size} -o {params.d} -t {threads} {params.overlap} {params.iterations}"

rule link_flye_assembly:
    input: "{uuid}.{conf}/assembly.fasta"
    output: "{uuid}.{conf,flye[A-z0-9._-]*}.ctg.cns.fa"
    shell: "ln -s {input} {output}"

rule install_bandage_linux:
    output: ok=touch("bandage.ok"), b="ware/bandage/Bandage",
    shell: "cd ware; wget https://github.com/rrwick/Bandage/releases/download/v0.8.1/Bandage_Ubuntu_static_v0_8_1.zip; unzip Bandage_Ubuntu_static_v0_8_1.zip -d bandage;"

rule prep_wtdbg2_gfa:
    input:
        dot="{uuid}.{conf,wtdbg2[A-z0-9_-]*}.ctg.dot.gz",
        assembly="{uuid}.{conf,wtdbg2[A-z0-9_-]*}.ctg.lay.gz"
    output: "{uuid}.{conf,wtdbg2[A-z0-9._-]*}.ctg.cns.gfa"
    shell: "zcat {input.dot} | perl git/wtdbg2/scripts/wtdbg-dot2gfa.pl > {output}"

#NOTE We reduce only the flye GFA because it has too many links to draw, and also the
# wtdbg2 graph isnt actually representative of the consensus so the contigs dont match up at all
rule prep_flye_gfa:
    input: "{uuid}.{conf,flye[A-z0-9_-]*}/assembly_graph.gfa"
    output: "{uuid}.{conf,flye[A-z0-9._-]*}.ctg.cns.gfa"
    shell: "python ../scripts/reduce_gfa.py {input} 50000 > {output}"

rule bandage_assembly:
    input:
        b="bandage.ok",
        gfa="{uuid}.{prefix}.gfa",
        assembly="{uuid}.{prefix}.fa"
    output: "{uuid}.{prefix}.gfa.svg"
    shell: "ware/bandage/Bandage image {input.gfa} {output}"

rule wtdbg2_consensus:
    input:
        "{uuid}.{conf}.ctg.lay.gz"
    output:
        "{uuid}.{conf,wtdbg2[A-z0-9_-]*}.ctg.cns.fa"
    threads: config["assembly_threads"]
    benchmark: "benchmarks/{uuid}.{conf,wtdbg2[A-z0-9_-]*}.ctg.cns.fa"
    shell:
        "git/{wildcards.conf}/wtdbg2/wtpoa-cns -f -i {input} -o {output} -t {threads}"

rule install_wtdbg2_hash:
    output:
        ok=touch("{conf,wtdbg2[A-z0-9_-]*}.ok"),
        #w2_bin="git/{conf,wtdbg2[A-z0-9_-]*}/wtdbg2/wtdbg2",
        #w2_poa="git/{conf,wtdbg2[A-z0-9_-]*}/wtdbg2/wtpoa-cns",
    params:
        hash=lambda w: spells[w.conf]["hash"],
    shell: "mkdir -p git/{wildcards.conf}; cd git/{wildcards.conf}; git clone https://github.com/ruanjue/wtdbg2.git; cd wtdbg2; git reset --hard {params.hash}; make;"

rule wtdbg2_assembly:
    input:
        reads=lambda w: get_reads(w, 'ont'),
        ready="w2.ok"
    params:
        pmer=lambda w: spells[w.conf]['pmer'],
        kmer=lambda w: spells[w.conf]['kmer'],
        sampler=lambda w: spells[w.conf]['sampler'],
        edge=lambda w: spells[w.conf]['edge'],
        length=lambda w: spells[w.conf]['length'],
        max_k=lambda w: spells[w.conf]['max_k'],
        max_node=lambda w: spells[w.conf]['max_node'],
        prefix=lambda w: w.uuid+'.'+w.conf,
    output:
        lay="{uuid}.{conf,wtdbg2}.ctg.lay.gz",
        dot="{uuid}.{conf,wtdbg2}.ctg.dot.gz",
    threads: config["assembly_threads"]
    benchmark: "benchmarks/{uuid}.{conf,wtdbg2}.ctg.lay.gz"
    shell:
        "git/{wildcards.conf}/wtdbg2/wtdbg2 -f -i {input.reads} -o {params.prefix} -S {params.sampler} -e {params.edge} -k {params.kmer} -p {params.pmer} -L {params.length} -K {params.max_k} --node-max {params.max_node} -t {threads}"

rule wtdbg2_24_assembly:
    input:
        reads=lambda w: get_reads(w, 'ont'),
        ready=lambda w: "%s.ok" % samples.loc[w.uuid]["spell"],
    params:
        prefix=lambda w: w.uuid+'.'+w.conf,
        pmer=lambda w: spells[w.conf]['pmer'],
        kmer=lambda w: spells[w.conf]['kmer'],
        sampler=lambda w: spells[w.conf]['sampler'],
        edge=lambda w: spells[w.conf]['edge'],
        length=lambda w: spells[w.conf]['length'],
        max_k=lambda w: spells[w.conf]['max_k'],
        max_node=lambda w: spells[w.conf]['max_node'],
        genome_size=lambda w: spells[w.conf]['genome_size'],
    output:
        lay="{uuid}.{conf,wtdbg2[A-z0-9_-]*}.ctg.lay.gz",
        dot="{uuid}.{conf,wtdbg2[A-z0-9_-]*}.ctg.dot.gz",
    threads: config["assembly_threads"]
    benchmark: "benchmarks/{uuid}.{conf,wtdbg2[A-z0-9_-]*}.ctg.lay.gz"
    shell:
        "git/{wildcards.conf}/wtdbg2/wtdbg2 -f -i {input.reads} -o {params.prefix} -S {params.sampler} -e {params.edge} -k {params.kmer} -p {params.pmer} -L {params.length} -K {params.max_k} --node-max {params.max_node} -X {params.max_k} -g {params.genome_size} -t {threads}"
